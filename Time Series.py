# -*- coding: utf-8 -*-
"""Submis Pengembang 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bWVmnlxqkFyMn28DRXCfXH5NlbbWFtIV
"""

from google.colab import drive
import numpy as np
import pandas as pd
from keras.layers import Dense, LSTM
import matplotlib.pyplot as plt
import tensorflow as tf

!pip install kaggle

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!ls ~/.kaggle

!kaggle datasets download -d mahirkukreja/delhi-weather-data

from zipfile import ZipFile
file_name = "delhi-weather-data.zip"

with ZipFile(file_name,'r') as zip:
  zip.extractall()
  print('Done')

df = pd.read_csv('testset.csv')
df.head()

df.shape

df.info()

df.isnull().sum()

df.columns

df.drop([' _conds', ' _dewptm', ' _fog', ' _hail',
       ' _heatindexm', ' _hum', ' _precipm', ' _pressurem', ' _rain', ' _snow',
       ' _thunder', ' _tornado', ' _vism', ' _wdird', ' _wdire',
       ' _wgustm', ' _windchillm', ' _wspdm'], axis=1, inplace=True)

display(df)

df['datetime_utc'] = pd.to_datetime(df['datetime_utc'])  
get_data = (df['datetime_utc'] > '2010-01-01') & (df['datetime_utc'] <= '2015-01-01')
df.loc[get_data]

df = df.loc[get_data]
display(df)

df.isnull().sum()

df.dropna(subset=['datetime_utc'],inplace=True)
df.dropna(subset=[' _tempm'],inplace=True)
df.isnull().sum()

df.info()

dates = df['datetime_utc'].values
tempm = df[' _tempm'].values

dates = np.array(dates)
temp = np.array(tempm)
plt.figure(figsize=(20,5))
plt.plot(dates, tempm)
plt.title('Average Temperature', fontsize = 20)
plt.ylabel('Temperature')
plt.xlabel('Datetime')

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[-1:]))
    return ds.batch(batch_size).prefetch(1)

from sklearn.model_selection import train_test_split

x_train, x_valid, y_train, y_valid = train_test_split(tempm, dates, train_size=0.8, test_size = 0.2, shuffle = False )
print(len(x_train), len(x_valid))

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM,Dense,Bidirectional,Dropout

tf.keras.backend.set_floatx('float64')

train_set = windowed_dataset(x_train, window_size=64, batch_size=200, shuffle_buffer=1000)
val_set = windowed_dataset(x_valid, window_size=64, batch_size=200, shuffle_buffer=1000)

model = tf.keras.models.Sequential([
    Bidirectional(LSTM(60, return_sequences=True)),
    Bidirectional(LSTM(60)),
    Dense(30, activation="relu"),
    Dense(10, activation="relu"),
    Dense(1),
])

Mae = (df[' _tempm'].max() - df[' _tempm'].min()) * 10/100
print(Mae)

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('mae')<4.2 and logs.get('val_mae')<4.2):
      print("\nMAE dari model < 10% skala data")
      self.model.stop_training = True
callbacks = myCallback()

optimizer = tf.keras.optimizers.SGD(lr=1.0000e-04, momentum=0.9)

model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])

history = model.fit(train_set, epochs=100, validation_data = val_set, callbacks=[callbacks])

import matplotlib.pyplot as plt
plt.plot(history.history['mae'])
plt.plot(history.history['val_mae'])
plt.title('MAE')
plt.ylabel('mae')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()